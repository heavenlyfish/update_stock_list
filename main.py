"""
æŠ“å–ä¸Šå¸‚ / ä¸Šæ«ƒ / èˆˆæ«ƒ ISINï¼Œåˆä½µå¾Œè²¼åˆ°
https://docs.google.com/spreadsheets/d/GSHEET_ID (å·¥ä½œè¡¨ã€Œä¸Šå¸‚æ«ƒã€)
"""
import os, json, requests, pandas as pd
import time
from bs4 import BeautifulSoup, FeatureNotFound
import gspread
import io
from gspread_dataframe import set_with_dataframe
from google.oauth2.service_account import Credentials


UA = (
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
    "AppleWebKit/537.36 (KHTML, like Gecko) "
    "Chrome/124.0.0.0 Safari/537.36"
)

BASE = "isin.twse.com.tw"

def fetch_twse(mode: int, headers: dict, use_https: bool = True) -> requests.Response:
    scheme = "https" if use_https else "http"
    url = f"{scheme}://{BASE}/isin/C_public.jsp?strMode={mode}"
    return requests.get(url, timeout=30, headers=headers)

def get_isin(mode: int,
             start_mark: str | None = None,
             end_mark: str | None = None,
             max_retry: int = 3,
             backoff: float = 2.0) -> pd.DataFrame:
    headers = {"User-Agent": UA}
    use_https = True            # å…ˆä»¥ HTTPS å˜—è©¦

    for attempt in range(1, max_retry + 1):
        # â‘  æŠ“è³‡æ–™ï¼ˆå¤±æ•—å°±è‡ªå‹•æ”¹ HTTPï¼Œå†é‡è©¦ï¼‰
        try:
            r = fetch_twse(mode, headers, use_https=use_https)
        except requests.RequestException as e:
            print(f"[warn] mode={mode} æŠ“å–å¤±æ•— ({e}); æ”¹ç”¨ HTTP")
            use_https = False
            time.sleep(backoff * attempt)
            continue

        r.encoding = "big5"
        soup = BeautifulSoup(r.text, "lxml")
        table = soup.select_one("table.h4")

        if table:                       # â‘¡ æˆåŠŸæŠ“åˆ° table
            break
        else:
            print(f"[warn] mode={mode} ç¬¬ {attempt}/{max_retry} æ¬¡æŠ“ä¸åˆ° table.h4")
            if attempt == 1:            # ç¬¬ä¸€æ¬¡æŠ“ä¸åˆ°å°±åˆ‡ HTTP å†è©¦
                use_https = False
            if attempt < max_retry:
                time.sleep(backoff * attempt)
    else:
        # â‘¢ ä¸‰æ¬¡éƒ½å¤±æ•— â†’ æœ€å¾Œç”¨ read_html è©¦è©¦
        try:
            df_list = pd.read_html(
                io.StringIO(r.text), header=0, encoding="big5", flavor="lxml"
            )
            if df_list:
                table_html = df_list[0].to_html(index=False)
                table = BeautifulSoup(table_html, "lxml").select_one("table")
        except ValueError:
            table = None

    if not table:
        print(f"[error] mode={mode} è§£æå¤±æ•—ï¼Œå›å‚³ç©º DataFrame")
        return pd.DataFrame(columns=["ä»£è™Ÿ", "ç°¡ç¨±", "å¸‚å ´åˆ¥", "ç”¢æ¥­åˆ¥"])

    # ---------- ä»¥ä¸‹èˆ‡åŸæœ¬é‚è¼¯ç›¸åŒ ----------
    head_tr = next(
        tr for tr in table.find_all("tr")
        if tr.find("td") and tr.find("td").get_text(strip=True).startswith("æœ‰åƒ¹è­‰åˆ¸ä»£è™Ÿ")
    )
    headers = [td.get_text(strip=True) for td in head_tr.find_all("td")]

    rows, collect = [], start_mark is None
    for tr in head_tr.find_next_siblings("tr"):
        tds = [td.get_text(strip=True) for td in tr.find_all("td")]

        if len(tds) == 1:  # åˆ†é¡åˆ—
            mark = tds[0]
            if start_mark and mark.startswith(start_mark):
                collect = True
                continue
            if end_mark and mark.startswith(end_mark):
                break
            continue

        if collect and len(tds) == len(headers):
            rows.append(tds)

    df = pd.DataFrame(rows, columns=headers)
    if df.empty:
        return pd.DataFrame(columns=["ä»£è™Ÿ", "ç°¡ç¨±", "å¸‚å ´åˆ¥", "ç”¢æ¥­åˆ¥"])

    df[["ä»£è™Ÿ", "ç°¡ç¨±"]] = (
        df["æœ‰åƒ¹è­‰åˆ¸ä»£è™ŸåŠåç¨±"]
          .str.replace("\u3000", " ", regex=False)
          .str.extract(r"^(\w+)\s+(.*)$")
    )
    return df[["ä»£è™Ÿ", "ç°¡ç¨±", "å¸‚å ´åˆ¥", "ç”¢æ¥­åˆ¥"]]


from datetime import datetime  # Add this import if not already at the top

def crawl_all() -> pd.DataFrame:
    twse = get_isin(2, start_mark="è‚¡ç¥¨", end_mark="ä¸Šå¸‚èªè³¼")
    otc  = get_isin(4, start_mark="è‚¡ç¥¨", end_mark="ç‰¹åˆ¥è‚¡")
    emg  = get_isin(5)
    if emg.empty:
        print("[warn] èˆˆæ«ƒæŠ“å–å¤±æ•—ï¼Œå°‡è·³éåˆä½µ")
        emg = pd.DataFrame(columns=["ä»£è™Ÿ", "ç°¡ç¨±", "å¸‚å ´åˆ¥", "ç”¢æ¥­åˆ¥"])

    # åˆä½µä¸‰å€‹å¸‚å ´
    df_all = (
        pd.concat([twse, otc, emg], ignore_index=True)
          .drop_duplicates(subset="ä»£è™Ÿ")
          .sort_values("ä»£è™Ÿ")
          .reset_index(drop=True)
    )

    # ğŸ•’ æ–°å¢ã€Œæ›´æ–°æ—¥ã€æ¬„ä½
    now_str = datetime.now().strftime("%Y-%m-%d %H:%M")
    df_all["æ›´æ–°æ—¥"] = now_str

    # å¦‚æœä½ æƒ³è¦ã€Œæ›´æ–°æ—¥ã€æ’ç¬¬ä¸€æ¬„ï¼ŒåŠ å…¥é€™è¡Œï¼š
    df_all = df_all[["æ›´æ–°æ—¥", "ä»£è™Ÿ", "ç°¡ç¨±", "å¸‚å ´åˆ¥", "ç”¢æ¥­åˆ¥"]]

    return df_all


def upload_to_gsheet(df: pd.DataFrame):
    gsheet_id = os.getenv("GSHEET_ID")
    if not gsheet_id:
        raise RuntimeError("ç’°å¢ƒè®Šæ•¸ GSHEET_ID æœªè¨­å®š")

    creds_json = os.getenv("GOOGLE_SERVICE_ACCOUNT_JSON")
    creds = Credentials.from_service_account_info(json.loads(creds_json),
                                                  scopes=[
                                                      "https://www.googleapis.com/auth/spreadsheets",
                                                      "https://www.googleapis.com/auth/drive",
                                                  ])
    gc = gspread.authorize(creds)
    sh = gc.open_by_key(gsheet_id)
    ws = sh.worksheet("ä¸Šå¸‚æ«ƒ")                        # å·¥ä½œè¡¨åç¨±

    ws.clear()                                        # æ¸…ç©ºèˆŠè³‡æ–™
    set_with_dataframe(ws, df, include_index=False)  # å¾ A1 é–‹å§‹è²¼


if __name__ == "__main__":

    df_all = crawl_all()
    
    out_dir  = "data"
    os.makedirs(out_dir, exist_ok=True)          # â† é—œéµï¼šè‡ªå‹•å»ºè³‡æ–™å¤¾
    out_path = os.path.join(out_dir, "stock_list.csv")

    df_all.to_csv(out_path, index=False, encoding="utf-8-sig")
    print(f"CSV saved to {out_path}")

    #upload_to_gsheet(df_all)                     # å¦‚æœä½ é‚„è¦è²¼ Google Sheets